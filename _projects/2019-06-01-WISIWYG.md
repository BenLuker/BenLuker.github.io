---
name: WISIWYG
tools: [TouchDesigner]
image: https://drive.google.com/uc?export=view&id=1GFSKs7EcgjKcIufHMdsvlNn7snQf2-u6
description: Debuted at the Getty Center, this installation named “WISIWYG" (What I See Is What You Get), produced live data and animations by analyzing user facial expressions and used a machine learning algorithm to train the artificial neural network models from the Getty Museum’s art collection. It now runs live at Woodbury University's School of Business building.
---

# WISISWYG
#### June 2019
___

<div>
    <video class="figure w-100" autoplay loop muted poster="https://drive.google.com/uc?export=view&id=16YtRdrje4qthwjVmIb_inF7tBdA_7yGP">
        <source src="https://drive.google.com/uc?export=view&id=1lpNXwmYQ5mCqwyc7S-hiZL_vtfbtj-Rj" type="video/mp4">
    </video>
</div>

<br>
I was given the opportunity to work on an installation for the Getty! I and three other students used a program we just learned at the beginning of the semester called TouchDesigner to create generative art.

My job was to make the "playback system", aka the state machine. It linked all of the different art pieces to create a cohesive experience. I also created the idle state for the installation, which was an iterative art piece that got a digital stroke added to it with each complete interaction. Everyone who used the installation added to the piece, and over the course of the night the piece began to take shape.

<div>
    <video class="figure w-100" autoplay loop muted poster="https://drive.google.com/uc?export=view&id=1zGla-9__5DAqHE3k_RMwemKtVVOPs8zG">
        <source src="https://drive.google.com/uc?export=view&id=1oxb0Be4C6b2fivuVUbJpa7SPDA5kYlc0" type="video/mp4">
    </video>
</div>

<br>
On April 29, 2019, the Getty Center celebrated “Color.” An event focused on color in imagination, the science of
color, art, and the future. The chair of our department, Ana Herruzo, gave us the
opportunity to create an immersive, interactive installation merging artificial intelligence and visual arts. Named
“WISIWYG:
What I See Is What You Get,” the installation produced live data and animations by analyzing user facial expressions
and used a
machine learning algorithm to train the artificial neural network models from the Getty Museum’s art collection. It
consisted of three video screens stacked vertically into one seamless display, a camera for facial recognition, and
an Xbox kinect for motion data. The experience was powered by TouchDesigner, a tool for creating realtime generative
art.

{% capture carousel_images %}
https://drive.google.com/uc?export=view&id=1GFSKs7EcgjKcIufHMdsvlNn7snQf2-u6
https://drive.google.com/uc?export=view&id=1-6YvVT0wNTxKuBJKCl60BzfC4BpZs-8m
https://drive.google.com/uc?export=view&id=1Fm8PUFpZFQKyCPQETTXyM6vfTaCQijTB
https://drive.google.com/uc?export=view&id=1kQOQa-B33TOxHJQA-_LjZ1evbWYRi2E7
https://drive.google.com/uc?export=view&id=1kTBsGmbTCVy_J6jGCoAubGzvpGrRHY5J
https://drive.google.com/uc?export=view&id=1LNgsM9117gJQ8XpXMOmF8SmT0hDEhIxL
{% endcapture %}
{% include elements/carousel.html %}

{% include elements/figure.html image="https://drive.google.com/uc?export=view&id=1n3rvz4a8b5NZKvC7Rm5jdCU3R_EIGiWn" caption="Applied Computer Science students and professors at the Getty Center" %}

{% include elements/figure.html image="https://drive.google.com/uc?export=view&id=181ecAnFQSDFolt0akvvNZFwkKKkwUXpr" caption="Art generated by the installation's idle state" %}